crossV = function(Matrix[double] X, Matrix[double] y, double lamda, Integer icpt=0, Integer k) return (Matrix[double] R)
{
  #create empty lists
  dataset_X = list(); #empty list
  dataset_y = list();
  fs = ceil(nrow(X)/k);
  off = fs - 1;
  #devide X, y into lists of k matrices
  for (i in seq(1, k)) {
    dataset_X = append(dataset_X, X[i*fs-off : min(i*fs, nrow(X)),]);
    dataset_y = append(dataset_y, y[i*fs-off : min(i*fs, nrow(y)),]);
  }

  beta_list = list();
  #keep one fold for testing in each iteration
  for (i in seq(1, k)) {
    [tmpX, testX] = remove(dataset_X, i);
    [tmpy, testy] = remove(dataset_y, i);
    trainX = rbind(tmpX);
    trainy = rbind(tmpy);
    trainX = trainX[,1:ncol(X)] # TODO improve list size propagation
    beta = lmDS(X=trainX, y=trainy, icpt=icpt, reg=lamda, verbose=FALSE);
    beta_list = append(beta_list, beta);
  }

  R = cbind(beta_list);
  R = rowSums(R);
}

l2norm = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] B, Integer icpt=0) 
return (Matrix[Double] loss) {
  if (icpt > 0)
    X = cbind(X, matrix(1, nrow(X), 1));
  loss = as.matrix(sum((y - X%*%B)^2));
}

M = $1;
X = rand(rows=M, cols=2500, seed=42); #Each fold up to ~15gb
y = rand(rows=M, cols=1, sparsity=1.0, seed=1);

k = 1;
Rbeta = matrix(0, rows=10, cols=ncol(X));
Rloss = matrix(0, rows=10, cols=1);
for (h in -9:0) {
  reg = 10^-h;
  beta = crossV(X=X, y=y, lamda=reg, k=4);
  Rbeta[k, 1:nrow(beta)] = t(beta);
  Rloss[k,] = l2norm(X=X, y=y, B=beta);
  k = k + 1;
}
leastLoss = rowIndexMin(t(Rloss));
bestModel = Rbeta[as.scalar(leastLoss),];
write(bestModel, "outdml", format="binary");
